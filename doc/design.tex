%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}
\graphicspath{{./}}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\usepackage{tikz} % Draw graph library

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Network Virtualization and Data Center Networks} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Assignment 2: Cloud computing - software engineer perspective\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Erik Henriksson, Christoph Burkhalter} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

\section{Proposal}

\subsection{Application}


We will use a simple web-server scenario for the benchmarking and the auto-scaling. There are two types of machines: load-balancers and worker machines. The load-balancers get an IP from a predefined pool of Elastic-IP's such that requests from clients can be sent to them. All load-balancers connect to the overlay network and receive the list of worker machines from the coordinator. Incoming request are sent to a worker machine using a load-balancing technique. In this way load-balancers are connected to all machines in the network, whereas worker machines are only connected to the load-balancers.
The worker machines process the request and contain no further application logic, which means they are not members of the overlay network.
The members of the overlay network (the load-balancers) elect a coordinator and perform a re-electing in case the coordinator dies. We will study the case with exactly two load-balancers, but it is of course easy to scale this up.

\tikzset{
  treenode/.style = {align=center, inner sep=0pt, text centered,
    font=\sffamily},
  load_balancer/.style = {circle, text=white, draw=none,fill=blue},
  node/.style = {circle, text=black, draw=black},
}

\begin{figure}[h!]
\begin{center}
\begin{tikzpicture}[node distance=1cm]
  
  \node [load_balancer] (l1) at (1,2) {L1};
  \node [load_balancer] (l2) at (3,2) {L2};
  \node [node] (n1) at (0,0)  {W1};
  \node [node] (n2) at (2,0) {W2};
  \node [node] (n3) at (4,0)  {W3};

  \foreach \from/\to in {l1/l2,l1/n1,l1/n2,l1/n3,l2/n1,l2/n2,l2/n3}
    \draw (\from) -- (\to);

\end{tikzpicture}
\end{center}

\caption{Network with load balancers (L1,L2) and processing nodes (W1,W2,W3)}
\end{figure}

\subsection{Benchmark}

The benchmark contains latency measurements between machines in the Amazon cloud (cloud to cloud), and a latency test of the application from a client outside the Amazon cloud (client to cloud). The benchmark will be initiated by the coordinator and executed on each machine for the performance benchmark. The latency benchmark is performed on the load-balancers; they test the latency on the network to all running machines in the cloud (other load balancers as well as all the worker machines). These results will be used to optimize load-balancing as well as for the auto scaling.
Additionally,the performance of the system is benchmarked by various workloads (e.g. 1000 request per second from a client). These benchmarks test will be performed with different numbers of worker machines, but a fixed number of two load-balancers.

\subsection{Auto-scaling}

The coordinator in the load balancer network performs the auto-scaling. If the load on the worker machines is too large, the coordinator will start new machines (scale-out). For the scale-out there will be a snapshot with the installed application to improve the start-up time. 
If the workload decreases, the reverse will be done.
In this part we will compare the Amazon load-balancer with our own implementation of a load-balancer, where we detect heavy-loaded machines if the latency increases or from periodically sending ping messages.

\section{Implementation}

We have used the twisted library in python and found a open source load balancer named \verb|txLoadBalancer| to do our implementation. It had functions for proxying requests, scheduling the processing nodes and working as a reverse proxy. To this we added the overlay code, code for measuring the request processing time and code to scale up and down the processing nodes.

\subsection{Overlay}
Our overlay network consists of a coordinator load balancer and slave load balancers. The coordinator is responsible for the autoscaling of the network and maintaining of the overlay, otherwise the load balancers are identical. Note that the processing nodes is not part of the overlay, they are not aware of anything else than process incoming requests from the load balancers. The overlay nodes has currently not any ability to start or stop each other, it is purely a this of redundancy. With this said, a couple of these load balancers can tolerate a very high number of requests per second and bottleneck should not be the load balancers. A couple of load balancers are also "cheap", whereas a huge number of processing nodes are not. We can therefore use use a safe margin when we dimension the number of load balancers without this having any important impact on the budget.

\subsection{Request processing time}
To get a good measurement of the network load we need to use a way to measure the amount of incoming request combined with their difficulty to process. A problem that arises here is that we can not assume anything of the distribution of request to different load balancers, and measuring at all of them is hard to synchronize. Therefore, we chose to measure the average processing time, since this is dependant on both the difficulty to process and the amount of requests. It is also very easy to measure at a central location, i.e. the coordinator load balancer. 

To avoid the problem when the load balancer does not get any reguests (and therefore can't calculate an average) we send dummy requests from the coordinator load balancer. The average is then calculated as
$$\frac{1}{N}\sum^N_{i=1} T_i$$
where $T_i$ is an exponential moving average of the request processing time at processing node $i$ and $N$ is the number of processing nodes.
\subsection{Autoscaling}
We use, as noted before, the average request processing time to determine when to scale up or down the processing node. It is the coordinator load balancer that is responsible for this, and it will have to config parameters to determine this, namely the scale-down parameter \verb|scale_down| and the scale-up parameter \verb|scale_up|. It simply compares the current average with the parameters and scales up if current average is above the \verb|scale_up| parameter. Scale down is done in the opposite way. On each scale we do doubling or halving of the network to achieve a certain size in $log(n)$ steps. It can therefore scale up or down very fast and will tolerate very fast changes in load without failing requests.

\section{Problems}
We noticed a couple of problems during our experiments on the Amazon EC2 cloud. We noticed that a start-up of a node can take up to 60 seconds, something that has impact on the speed of scaling. This problem can be seen in the graphs, where the load balancer sees a high load and initiates a scale up but it takes up to 60 seconds for this to take effect. 

We also had problems with CPU throttling on the micro instances, where you are granted a "free" CPU burst for some minutes and then restricted to a very low amount of CPU. This makes the average request processing time skyrocket even faster and the load balancer sees this as a increased load and scales up the nodes even faster. We therefore do not recommend running this on micro instances, it is better to use small instances where the CPU capacity is guaranteed over time.
\section{Results}

Result description latency (follows...)

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/latency_fixed.png}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/latency_fixed_small.png}
\end{figure*}

Result description workload (follows...)

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/workload.png}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/workload_small.png}
\end{figure*}

Result description timeline (follows...)

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/timeline.png}
\end{figure*}

\begin{figure*}[h!]
\includegraphics[width=\columnwidth]{../plot/timeline_small.png}
\end{figure*}




\end{document}
